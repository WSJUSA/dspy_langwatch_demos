{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f2e88c9",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title: \"Observe DSPy's CORPO Optimizer with LangWatch - Tutorial\"\n",
    "author: Will James\n",
    "date: 2025-07-09\n",
    "description: This tutorial explains DSPy's COPRO prompt optimizer and how to use LangWatch to observe its internal processes. This tutorial targets DSPy beginners, providing an introduciton to how DSPy optimizers work using clear step by step setup instructions.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e92759b",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "When first using DSPy optimizers, it's common to feel uncertain about their internal operations, effectiveness, configuration choices, LLM call volumes, etc.\n",
    "\n",
    "When I started with DSPy I was always concerned I was doing something wrong that would just waste money on LLM calls and miss out on getting better results.\n",
    "\n",
    "For me, I wanted to examine the optimizers in depth to fully understand how they work, leading me to seek observation tools that could trace their execution. I settled on LangWatch, a logging and dashboard tool; but its integration with DSPy was also non-trivial to set up for full visibility.\n",
    "\n",
    "**In this tutorial I walk through details of using DSPy COPRO Optimizer and LangWatch and hopefully this can help you accelerate using both.**\n",
    "\n",
    "# Key Concepts\n",
    "\n",
    "[**DSPY COPRO optimizer**](https://dspy.ai/api/optimizers/COPRO/) is a basic prompt optimizer. It serves as an entry point to DSPy optimizers. \n",
    "\n",
    "[**LangWatch**](https://langwatch.ai/) is an open-source tool for Prompt and Agent observation; it is installable locally. [You can get LangWatch for local installation here](https://github.com/langwatch/langwatch?tab=readme-ov-file#self-hosted-%EF%B8%8F).\n",
    "\n",
    "# Tutorial Steps Overview\n",
    "\n",
    "1. Create a prompt with DSPy that scores data.\n",
    "2. Load a dataset we will use to evaluate and optimize the prompt.\n",
    "3. Run a baseline evaluation of the prompt using the cheapest LLM model.\n",
    "3. Run the COPRO optimizer on the prompt.\n",
    "4. Evaluate the optimized prompt.\n",
    "5. Along the way log all DSPy actions above to LangWatch to inspect COPRO's operations, including prompts, iterations, and costs.\n",
    "\n",
    "COPRO augments prompts only, keeping setup simple to focus on connecting DSPy with LangWatch and observing DSPy's processes. \n",
    "\n",
    "This tutorial uses Python and assumes you can install libraries or get a code agent to do it for you.\n",
    "\n",
    "# Python Imports\n",
    "Lets start with imports we need, nothing special here except some pathing for jupter on my Windoze machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfab4a7-4134-4765-bc0f-d55a9449588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import dspy\n",
    "import langwatch\n",
    "from dotenv import load_dotenv\n",
    "from dspy.evaluate import Evaluate\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Automatically add the project root to sys.path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Keys are kept in .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabe4600",
   "metadata": {},
   "source": [
    "# DSPy Signature and Module, and Metric\n",
    "\n",
    "First we need a DSPy managed prompt. This is what we will optimize.\n",
    "\n",
    "This code declares our DSPy Signature and Module classes. If you are not familiar with these then go first read the basics on using DSPy [Singnatures](https://dspy.ai/learn/programming/signatures/) and [Modules](https://dspy.ai/learn/programming/modules/).\n",
    "\n",
    "The Singature here asks the LLM to evaluate if a given sentence is grammatically correct or not.\n",
    "\n",
    "This code also sets up a metric, which is how we score the LLM's response to our prompt. The metric simply comparies the LLM response (0,1) to the known correct answer ```label```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849af12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define signature\n",
    "class GrammaticalitySignature(dspy.Signature):\n",
    "    \"\"\"Classify if the sentence is grammatically correct (1) or not (0).\"\"\" #<-- this is the prompt we will optimize\n",
    "    sentence = dspy.InputField()\n",
    "    label = dspy.OutputField(desc=\"1 if correct, 0 if incorrect\")\n",
    "\n",
    "# Define module\n",
    "class GrammaticalityClassifier(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.predict = dspy.Predict(GrammaticalitySignature)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        return self.predict(sentence=sentence)\n",
    "\n",
    "# Define validation metric - evaluate if a LLM model predicts the grammar score (correct grammar or not) accurately.\n",
    "def custom_metric(example, pred, trace=None):\n",
    "    return example.label == pred.label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faecdcb2",
   "metadata": {},
   "source": [
    "# Training Dataset Setup\n",
    "\n",
    "Next we need some data. \n",
    "\n",
    "Training requires data and I wanted this tutorial to use a dataset you can just fetch and not have to generate or collect.  Luckily I found this [obscure example from Maylad31 on Github](https://github.com/maylad31/dspy-phi3) which I borrowed from for the foundation for this tutorial.\n",
    "\n",
    "## The CoLA Grammar Dataset\n",
    "The dataset used here is the CoLA grammar training set. It is a set of correct and incorrect grammar examples with labels for the correctness (0,  1). \n",
    "\n",
    "Its rows look like this, Label indicates if the sentence is grammatically correct or not:\n",
    "\n",
    "| Sentence | Label |\n",
    "|----------|------|\n",
    "| The book what inspired them was very long. | 0 |\n",
    "| This flyer and that flyer differ apart. | 0 |\n",
    "| Anson believed himself to be handsome. | 1 |\n",
    "| John is sad. | 1 |\n",
    "\n",
    "\n",
    "## Prepare the Dataset for Evaluating and Training our Prompt\n",
    "The code below will fetch it for free directly from HuggingFace and prepare it for our uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb26c2b8-09b9-46c1-8da6-2cae4514366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CoLA dataset (Corpus of Linguistic Acceptability)\n",
    "# Source https://huggingface.co/datasets/linxinyuan/cola/tree/main\n",
    "dataset = load_dataset(\"glue\", \"cola\")\n",
    "train_df = dataset['train'].to_pandas()\n",
    "val_df = dataset['validation'].to_pandas()\n",
    "\n",
    "# Shuffle and truncate a subset of the training split for demonstration\n",
    "train_df = train_df.sample(frac=1, random_state=23).head(10).reset_index(drop=True)\n",
    "# Shuffle and truncate a subset of the test split for demonstration\n",
    "val_df = val_df.sample(frac=1, random_state=23).head(300).reset_index(drop=True)\n",
    "\n",
    "# Create trainset and devset\n",
    "trainset = [dspy.Example(sentence=ex['sentence'], label=str(ex['label'])).with_inputs('sentence') for ex in train_df.to_dict(orient='records')]\n",
    "devset = [dspy.Example(sentence=ex['sentence'], label=str(ex['label'])).with_inputs('sentence') for ex in val_df.to_dict(orient='records')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74143303",
   "metadata": {},
   "source": [
    "# Three Steps to Optimization\n",
    "\n",
    "Using COPRO there will be three steps to optimizing the prompt in the Signaure.\n",
    "\n",
    "1. **Baseline evaluation**: we will evaluate the nano model against the devset to see how well it does with the Signature we wrote.\n",
    "2. **Run COPRO**: COPRO will try to optmize the prompt in the Signature.\n",
    "3. **Apply the optimized Signature against an error set**: using any failed predictions from step one, we will run them again to see if our optimized prompt does any better. \n",
    "\n",
    "We will log all runs in these steps to LangWatch Experiments so we can see details, costs, get a clear view of how things work in DSPy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2e1cd7",
   "metadata": {},
   "source": [
    "## DSPy Configuration\n",
    "\n",
    "Before we get into the steps we need a DSPy configuration. We will use it in all 3 Steps.\n",
    "\n",
    "DSPY here is configured with OpenAI's 4.1-nano model to see what kind of results we can get with the cheapest model.  \n",
    "\n",
    "This is the default model in our configuration.  We will use it multiple times. Any DSPy LLM calls we run will use it, unless we specify otherwise.  \n",
    "\n",
    "_It is important to keep track of what model is in use when running optimizers._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d1d81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure DSPy - default model setup to use the cheapest available 4.1-nano from OpenAI\n",
    "lm = dspy.LM(model=\"openai/gpt-4.1-nano\", api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "dspy.settings.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeeab7d",
   "metadata": {},
   "source": [
    "## Step 1 - Baseline Evaluation of 4.1-nano\n",
    "\n",
    "We first will evaluate the prompt and the 4.1-nano model using the \"devset\" which is 300 samples so we can get a good sized error set.\n",
    "\n",
    "**This code snip also shows setting up LangWatch to observe the evaluation run.**\n",
    "\n",
    "_Note: I had to update the LangWatch DSPy extension to handle logging of DSPy Evaluate runs - that change has been accepted into LangWatch and should be available in the release. If you install the release and its not there yet, you can find the [LangWatch DSPy modification here](https://github.com/langwatch/langwatch/issues/484)._\n",
    "\n",
    "Watch out for the Langwatch ```experiment``` property in ```langwatch.dspy.init```.\n",
    "\n",
    "The experiment property sets the name of the experiment you will see in the Langwatch dashboard.  Since we will end up logging 3 different major exercises, we want to have a good naming convention to track what each experiment was for.\n",
    "\n",
    "This experiment is named \"grammar-4.1-nano-base-eval\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bee673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-22 09:49:41,711 - langwatch.client - INFO - Registering atexit handler to flush tracer provider on exit\n",
      "2025-07-22 09:49:41,712 - langwatch.client - WARNING - An existing global trace provider was found. LangWatch will not override it automatically, but instead is attaching another span processor and exporter to it. You can disable this warning by setting `ignore_global_tracer_provider_override_warning` to `True`.\n",
      "\n",
      "[LangWatch] `dspy.evaluate.Evaluate` object detected and patched for live tracking.\n",
      "\n",
      "[LangWatch] Experiment initialized, run_id: judicious-capuchin-of-thunder\n",
      "[LangWatch] Open http://localhost:5560/dspy-config-r1AguN/experiments/grammar-41-nano-base-eval?runIds=judicious-capuchin-of-thunder to track your DSPy training session live\n",
      "\n",
      "Average Metric: 238.00 / 300 (79.3%): 100%|██████████| 300/300 [00:00<00:00, 1854.58it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/22 09:49:42 INFO dspy.evaluate.evaluate: Average Metric: 238 / 300 (79.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>example_label</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>wrapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The book what inspired them was very long.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This flyer and that flyer differ apart.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anson believed himself to be handsome.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>John is sad.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Any albino tiger has orange fur, marked with black stripes.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The cat were bitten by the dog.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I squeaked the door.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Some people consider the dogs dangerous.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>It is obvious that Pat is lying.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Extremely frantically, Anson danced at Trade</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      sentence example_label  \\\n",
       "0                   The book what inspired them was very long.             0   \n",
       "1                      This flyer and that flyer differ apart.             0   \n",
       "2                       Anson believed himself to be handsome.             1   \n",
       "3                                                 John is sad.             1   \n",
       "4  Any albino tiger has orange fur, marked with black stripes.             1   \n",
       "5                              The cat were bitten by the dog.             0   \n",
       "6                                         I squeaked the door.             0   \n",
       "7                     Some people consider the dogs dangerous.             1   \n",
       "8                             It is obvious that Pat is lying.             1   \n",
       "9                 Extremely frantically, Anson danced at Trade             1   \n",
       "\n",
       "  pred_label    wrapped  \n",
       "0          0  ✔️ [True]  \n",
       "1          0  ✔️ [True]  \n",
       "2          1  ✔️ [True]  \n",
       "3          1  ✔️ [True]  \n",
       "4          0             \n",
       "5          0  ✔️ [True]  \n",
       "6          1             \n",
       "7          1  ✔️ [True]  \n",
       "8          1  ✔️ [True]  \n",
       "9          0             "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div style='\n",
       "                text-align: center;\n",
       "                font-size: 16px;\n",
       "                font-weight: bold;\n",
       "                color: #555;\n",
       "                margin: 10px 0;'>\n",
       "                ... 290 more rows not displayed ...\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define evaluation of the test/dev set to see how well the cheapestmodel performs without optimization\n",
    "evaluator = Evaluate(devset=devset, num_threads=4, display_progress=True, display_table=10, return_outputs=True)\n",
    "\n",
    "# Initialize Langwatch\n",
    "try:\n",
    "    langwatch.setup(\n",
    "        api_key=os.environ.get(\"LANGWATCH_API_KEY\"),\n",
    "        endpoint_url=os.environ.get(\"LANGWATCH_ENDPOINT\")\n",
    "    )\n",
    "# If Langwatch setup fails exit so we do not run llm calls without observability\n",
    "except Exception as e:\n",
    "    print(f\"LangWatch setup failed: {e}\")\n",
    "    sys.exit(1)  \n",
    "langwatch.dspy.init(\n",
    "    experiment=\"grammar-4.1-nano-base-eval\",\n",
    "    optimizer=None,\n",
    "    evaluator=evaluator #<-- pass the evaluator for logging to LangWatch-Evaluations\n",
    ") \n",
    "\n",
    "# Compile classifier module\n",
    "classifier = GrammaticalityClassifier()\n",
    "\n",
    "# Run DSPy evaluation and get the results\n",
    "score, results = evaluator(classifier, metric=custom_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfea8810",
   "metadata": {},
   "source": [
    "**Evaluation Results**\n",
    "\n",
    "The entire devset has been scored as you can sort of see in the terminal log above. The LLMs are getting so good that we get 79% accuracy out of the box with the lowest model.\n",
    "\n",
    "The DSPy Evalate terminal log table above is quite nice visually, however its not a longterm record and cannot be referenced easily.\n",
    "\n",
    "In LangWatch we get the full trace of the Evaluate run. Including the token cost from OpenAI. There is only one \"Step\" here (shown as the dot on the score graph) because the process did not have any iteration variations.\n",
    "\n",
    "**LangWatch views:**\n",
    "\n",
    "![*Figure 1: LangWatch Experiment Detail of 4.1-nano Evaluation.*](assets/copro/copro_lw_base_eval.png)\n",
    "\n",
    "In LangWatch we can see the prompt detail of every individual LLM call on the devset. You can see how DSPy is converting the Signature and Module to a POST to the LLM.  Notice we can see it is using the original Singature language.\n",
    "\n",
    "![*Figure 2: LangWatch Prompt Detail.*](assets/copro/copro_lw_base_eval_prompt.png)\n",
    "\n",
    "Knowing the nano model is pretty good, I segregate just the failures (mismatches) out. We will use this errors set after optimizing the prompt to see if we can get any improvement on just the hardest sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d398c310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 62 error items to cola_grammar_errors.csv\n"
     ]
    }
   ],
   "source": [
    "# Each item in eval_result.results is a tuple: (example, prediction, score)\n",
    "mismatches = [\n",
    "    {\n",
    "        \"sentence\": ex.sentence,\n",
    "        \"true_label\": ex.label,\n",
    "        \"predicted_label\": pred.label,\n",
    "        \"is_correct\": metric_result\n",
    "    }\n",
    "    for ex, pred, metric_result in results\n",
    "    if not metric_result\n",
    "]\n",
    "\n",
    "# Extract the LLM mismatched items to a CSV file as a set for optimiized analysis\n",
    "if mismatches:\n",
    "    pd.DataFrame(mismatches).to_csv(\"cola_grammar_errors.csv\", index=False, quoting=csv.QUOTE_ALL)\n",
    "    print(f\"Saved {len(mismatches)} error items to cola_grammar_errors.csv\")\n",
    "else:\n",
    "    print(\"No errors found in evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2ea271",
   "metadata": {},
   "source": [
    "## Step 2 - Run the COPRO Optimization\n",
    "\n",
    "Let's run the COPRO Optimization. \n",
    "\n",
    "There are few key settings to be aware of that will influence how many LLM calls happen in the optimization.\n",
    "\n",
    "1. trainset size - the number of rows from the dataset up top will be the number of runs per optimization iteration\n",
    "2. breadth - in the optimizer settings below this will be the number of prompt variations that COPRO will generate and test\n",
    "3. depth - this is the number of iterations per prompt COPRO will run.\n",
    "\n",
    "trainset x breadth x depth = total number of LLM runs.  \n",
    "\n",
    "**These can add up to hundreds of LLM requests quickly and may not add much value.** \n",
    "\n",
    "Depending on your use case you will need to experiment to find the best balance for performance vs cost. For this example we have a testset of just 10, and breadth of 3 and depth of 1 to be able to see what is going on with COPRO.\n",
    "\n",
    "Notice we change the experiment name in the langwatch init.\n",
    "\n",
    "Also notice we create a new DSPy LM and set it to 4o-mini -- this is to use a higher model for prompt variation recommendations that COPRO will request.\n",
    "\n",
    "Finally the last line, we save the optimization to a JSON file. You can see the modified prompt COPRO selects. This file is important for capturing the optimization and reloading it for long term use with DSPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20eecda-7334-442b-bf81-3143547ec583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-22 10:21:17,312 - langwatch.client - INFO - Registering atexit handler to flush tracer provider on exit\n",
      "2025-07-22 10:21:17,313 - langwatch.client - WARNING - An existing global trace provider was found. LangWatch will not override it automatically, but instead is attaching another span processor and exporter to it. You can disable this warning by setting `ignore_global_tracer_provider_override_warning` to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/22 10:21:17 INFO dspy.teleprompt.copro_optimizer: Iteration Depth: 1/1.\n",
      "2025/07/22 10:21:17 INFO dspy.teleprompt.copro_optimizer: At Depth 1/1, Evaluating Prompt Candidate #1/3 for Predictor 1 of 1.\n",
      "2025/07/22 10:21:17 INFO dspy.evaluate.evaluate: Average Metric: 9 / 10 (90.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LangWatch] Experiment initialized, run_id: imperial-cautious-pig\n",
      "[LangWatch] Open http://localhost:5560/dspy-config-r1AguN/experiments/grammar-41-nano-copro-train?runIds=imperial-cautious-pig to track your DSPy training session live\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-07-22T10:21:17.662141]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `basic_instruction` (str): The initial instructions before optimization\n",
      "Your output fields are:\n",
      "1. `proposed_instruction` (str): The improved instructions for the language model\n",
      "2. `proposed_prefix_for_output_field` (str): The string at the end of the prompt, which will help the model start solving the task\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "{basic_instruction}\n",
      "\n",
      "[[ ## proposed_instruction ## ]]\n",
      "{proposed_instruction}\n",
      "\n",
      "[[ ## proposed_prefix_for_output_field ## ]]\n",
      "{proposed_prefix_for_output_field}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        You are an instruction optimizer for large language models. I will give you a ``signature`` of fields (inputs and outputs) in English. Your task is to propose an instruction that will lead a good language model to perform the task well. Don't be afraid to be creative.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "Classify if the sentence is grammatically correct (1) or not (0).\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## proposed_instruction ## ]]`, then `[[ ## proposed_prefix_for_output_field ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## proposed_instruction ## ]]\n",
      "Evaluate the grammatical correctness of the provided sentence. Output \"1\" if the sentence is grammatically correct and \"0\" if it is not. Ensure your responses are clear and concise.\n",
      "\n",
      "[[ ## proposed_prefix_for_output_field ## ]]\n",
      "Classify the sentence: \n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\u001b[31m \t (and 1 other completions)\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/22 10:21:18 INFO dspy.teleprompt.copro_optimizer: At Depth 1/1, Evaluating Prompt Candidate #2/3 for Predictor 1 of 1.\n",
      "2025/07/22 10:21:18 INFO dspy.evaluate.evaluate: Average Metric: 9 / 10 (90.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-07-22T10:21:17.662141]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `basic_instruction` (str): The initial instructions before optimization\n",
      "Your output fields are:\n",
      "1. `proposed_instruction` (str): The improved instructions for the language model\n",
      "2. `proposed_prefix_for_output_field` (str): The string at the end of the prompt, which will help the model start solving the task\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "{basic_instruction}\n",
      "\n",
      "[[ ## proposed_instruction ## ]]\n",
      "{proposed_instruction}\n",
      "\n",
      "[[ ## proposed_prefix_for_output_field ## ]]\n",
      "{proposed_prefix_for_output_field}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        You are an instruction optimizer for large language models. I will give you a ``signature`` of fields (inputs and outputs) in English. Your task is to propose an instruction that will lead a good language model to perform the task well. Don't be afraid to be creative.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "Classify if the sentence is grammatically correct (1) or not (0).\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## proposed_instruction ## ]]`, then `[[ ## proposed_prefix_for_output_field ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## proposed_instruction ## ]]\n",
      "Evaluate the grammatical correctness of the provided sentence. Output \"1\" if the sentence is grammatically correct and \"0\" if it is not. Ensure your responses are clear and concise.\n",
      "\n",
      "[[ ## proposed_prefix_for_output_field ## ]]\n",
      "Classify the sentence: \n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\u001b[31m \t (and 1 other completions)\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/22 10:21:18 INFO dspy.teleprompt.copro_optimizer: At Depth 1/1, Evaluating Prompt Candidate #3/3 for Predictor 1 of 1.\n",
      "2025/07/22 10:21:18 INFO dspy.evaluate.evaluate: Average Metric: 9 / 10 (90.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-07-22T10:21:17.662141]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `basic_instruction` (str): The initial instructions before optimization\n",
      "Your output fields are:\n",
      "1. `proposed_instruction` (str): The improved instructions for the language model\n",
      "2. `proposed_prefix_for_output_field` (str): The string at the end of the prompt, which will help the model start solving the task\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "{basic_instruction}\n",
      "\n",
      "[[ ## proposed_instruction ## ]]\n",
      "{proposed_instruction}\n",
      "\n",
      "[[ ## proposed_prefix_for_output_field ## ]]\n",
      "{proposed_prefix_for_output_field}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        You are an instruction optimizer for large language models. I will give you a ``signature`` of fields (inputs and outputs) in English. Your task is to propose an instruction that will lead a good language model to perform the task well. Don't be afraid to be creative.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "Classify if the sentence is grammatically correct (1) or not (0).\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## proposed_instruction ## ]]`, then `[[ ## proposed_prefix_for_output_field ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## proposed_instruction ## ]]\n",
      "Evaluate the grammatical correctness of the provided sentence. Output \"1\" if the sentence is grammatically correct and \"0\" if it is not. Ensure your responses are clear and concise.\n",
      "\n",
      "[[ ## proposed_prefix_for_output_field ## ]]\n",
      "Classify the sentence: \n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\u001b[31m \t (and 1 other completions)\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-07-22T10:21:17.662141]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `basic_instruction` (str): The initial instructions before optimization\n",
      "Your output fields are:\n",
      "1. `proposed_instruction` (str): The improved instructions for the language model\n",
      "2. `proposed_prefix_for_output_field` (str): The string at the end of the prompt, which will help the model start solving the task\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "{basic_instruction}\n",
      "\n",
      "[[ ## proposed_instruction ## ]]\n",
      "{proposed_instruction}\n",
      "\n",
      "[[ ## proposed_prefix_for_output_field ## ]]\n",
      "{proposed_prefix_for_output_field}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        You are an instruction optimizer for large language models. I will give you a ``signature`` of fields (inputs and outputs) in English. Your task is to propose an instruction that will lead a good language model to perform the task well. Don't be afraid to be creative.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## basic_instruction ## ]]\n",
      "Classify if the sentence is grammatically correct (1) or not (0).\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## proposed_instruction ## ]]`, then `[[ ## proposed_prefix_for_output_field ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## proposed_instruction ## ]]\n",
      "Evaluate the grammatical correctness of the provided sentence. Output \"1\" if the sentence is grammatically correct and \"0\" if it is not. Ensure your responses are clear and concise.\n",
      "\n",
      "[[ ## proposed_prefix_for_output_field ## ]]\n",
      "Classify the sentence: \n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\u001b[31m \t (and 1 other completions)\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configure optimizer\n",
    "# COPRO generates new prompt va, specify a higher model for this - here we use gpt-4o-mini\n",
    "prompt_lm = dspy.LM(model=\"openai/gpt-4o-mini\", api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "optimizer = dspy.COPRO(\n",
    "    prompt_model=prompt_lm,\n",
    "    metric=custom_metric,\n",
    "    breadth=3,  #<-- number of prompts to generate\n",
    "    depth=1,    #<-- number of iterations (iteration = one prompt variation x tainset size)\n",
    "    init_temperature=1.4\n",
    ")\n",
    "\n",
    "# Initialize Langwatch again for the optimizer this time\n",
    "try:\n",
    "    langwatch.setup(\n",
    "        api_key=os.environ.get(\"LANGWATCH_API_KEY\"),\n",
    "        endpoint_url=os.environ.get(\"LANGWATCH_ENDPOINT\")\n",
    "    )\n",
    "# If Langwatch setup fails exit so we do not run llm calls without observability\n",
    "except Exception as e:\n",
    "    print(f\"LangWatch setup failed: {e}\")\n",
    "    sys.exit(1)  \n",
    "langwatch.dspy.init(experiment=\"grammar-4.1-nano-copro-train\", optimizer=optimizer)\n",
    "\n",
    "# Compile e.g. run the optimzer\n",
    "compiled_module = optimizer.compile(GrammaticalityClassifier(), trainset=trainset, eval_kwargs={\"num_threads\": 4})\n",
    "\n",
    "# Save the optimized module (GrammaticalityClassifier) to a json file\n",
    "compiled_module.save(\"grammatically_optimized.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c89142",
   "metadata": {},
   "source": [
    "**Optimization Results**\n",
    "\n",
    "With a little better prompt language we get some improvement in grammar correctness identification. That's pretty surprising, althrough the trainset is small at 10 rows.\n",
    "\n",
    "We can see each variation of 3 in LangWatch. Each is a Step. Clicking the dot on the graph will load the result set in the table below.  Notice each step makes 10 LLM calls from the training set plus extras for Optimization requests.\n",
    "\n",
    "![*Figure 3: Copro Training Steps in LangWatch.*](assets/copro/copro_lw_train.png)\n",
    "\n",
    "Here is the Copro optimizer asking the LLM for better prompt language. Note how it calls the higher model we set for this purpose.\n",
    "\n",
    "![*Figure 4: Copro Optimization Prompt in LangWatch.*](assets/copro/copro_lw_train_prompt.png)\n",
    "\n",
    "Further in LLM Calls we can see the new prompt sent by DSPy to the LLM to test its effectiveness.  Here we see a new prompt variation.\n",
    "\n",
    "![*Figure 5: Copro Training Step Promptin LangWatch.*](assets/copro/copro_lw_train_prompt_test.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26225372",
   "metadata": {},
   "source": [
    "## Step 3 - Evaluate the Optimization\n",
    "\n",
    "COPRO has already selected the best result for the saved optimization and we had a score of 90% accuracy across the trainset.  Lets check it against just the error set from the initial evaluation.\n",
    "\n",
    "This shows how to reload the optimized model JSON file and setup another evaluation run using the saved error set.\n",
    "\n",
    "Notice we change the experiment name in the langwatch init a third time, and pass the errorset to Evaluate. The DSPy foundational setup is still 4.1-nano, that never changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a08573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-22 13:16:09,524 - langwatch.client - INFO - Registering atexit handler to flush tracer provider on exit\n",
      "2025-07-22 13:16:09,526 - langwatch.client - WARNING - An existing global trace provider was found. LangWatch will not override it automatically, but instead is attaching another span processor and exporter to it. You can disable this warning by setting `ignore_global_tracer_provider_override_warning` to `True`.\n",
      "\n",
      "[LangWatch] `dspy.evaluate.Evaluate` object detected and patched for live tracking.\n",
      "\n",
      "[LangWatch] Experiment initialized, run_id: original-donkey-from-avalon\n",
      "[LangWatch] Open http://localhost:5560/dspy-config-r1AguN/experiments/grammar-41-nano-copro-eval?runIds=original-donkey-from-avalon to track your DSPy training session live\n",
      "\n",
      "Average Metric: 3.00 / 62 (4.8%): 100%|██████████| 62/62 [00:00<00:00, 725.94it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/22 13:16:10 INFO dspy.evaluate.evaluate: Average Metric: 3 / 62 (4.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>example_label</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>wrapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Any albino tiger has orange fur, marked with black stripes.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I squeaked the door.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Extremely frantically, Anson danced at Trade</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>She asked was Alison coming to the party.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Either Sam plays the bassoon or Jekyll the oboe.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What did you leave before they did?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Chris handed Bo.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The man who Mary loves and Sally hates computed my tax.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>John heard that they criticized themselves.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Over the fire there bubbled a fragrant stew.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>✔️ [True]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      sentence example_label  \\\n",
       "0  Any albino tiger has orange fur, marked with black stripes.             1   \n",
       "1                                         I squeaked the door.             0   \n",
       "2                 Extremely frantically, Anson danced at Trade             1   \n",
       "3                    She asked was Alison coming to the party.             1   \n",
       "4             Either Sam plays the bassoon or Jekyll the oboe.             1   \n",
       "5                          What did you leave before they did?             0   \n",
       "6                                             Chris handed Bo.             0   \n",
       "7      The man who Mary loves and Sally hates computed my tax.             1   \n",
       "8                  John heard that they criticized themselves.             0   \n",
       "9                 Over the fire there bubbled a fragrant stew.             1   \n",
       "\n",
       "  pred_label    wrapped  \n",
       "0          0             \n",
       "1          1             \n",
       "2          0             \n",
       "3          0             \n",
       "4          0             \n",
       "5          1             \n",
       "6          1             \n",
       "7          0             \n",
       "8          1             \n",
       "9          1  ✔️ [True]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div style='\n",
       "                text-align: center;\n",
       "                font-size: 16px;\n",
       "                font-weight: bold;\n",
       "                color: #555;\n",
       "                margin: 10px 0;'>\n",
       "                ... 52 more rows not displayed ...\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4.84"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the optimized classifier state\n",
    "loaded_classifier = GrammaticalityClassifier()  # Recreate the same program with the custom module.\n",
    "loaded_classifier.load(\"./grammatically_optimized.json\")\n",
    "\n",
    "# Load the errors dataset\n",
    "error_df = pd.read_csv(\"cola_grammar_errors.csv\")\n",
    "\n",
    "# Create a new devset from the mismatches\n",
    "# (Assuming your columns are: sentence, true_label, predicted_label, is_correct)\n",
    "errorset = [\n",
    "    dspy.Example(sentence=row['sentence'], label=str(row['true_label'])).with_inputs('sentence')\n",
    "    for _, row in error_df.iterrows()\n",
    "]\n",
    "\n",
    "# Evaluate the optimized classifier\n",
    "optimized_evaluator = Evaluate(devset=errorset, num_threads=1, display_progress=True, display_table=10)\n",
    "\n",
    "# Initialize Langwatch again\n",
    "try:\n",
    "    langwatch.setup(\n",
    "        api_key=os.environ.get(\"LANGWATCH_API_KEY\"),\n",
    "        endpoint_url=os.environ.get(\"LANGWATCH_ENDPOINT\")\n",
    "    )\n",
    "# If Langwatch setup fails exit so we do not run llm calls without observability\n",
    "except Exception as e:\n",
    "    print(f\"LangWatch setup failed: {e}\")\n",
    "    sys.exit(1)  \n",
    "langwatch.dspy.init(experiment=\"grammar-4.1-nano-copro-eval\", optimizer=None, evaluator=optimized_evaluator) #<-- pass the evaluator for logging to LangWatch-Evaluations\n",
    "\n",
    "# Run DSPy evaluation\n",
    "optimized_evaluator(loaded_classifier, metric=custom_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f320ede",
   "metadata": {},
   "source": [
    "From the prompt optimization, we get a 4.8% improvement on the hardest grammar examples. That's not incredible, but it is better than nothing if we're handling millions of requests. Also the example is to show tooling DSPy and LangWatch more than the best possible optimization.\n",
    "\n",
    "Again in LangWatch we get our log as before under a separate experiment and done in one Step with 62 LLM calls, one for each in our error set. DSPy selects the most effective prompt from the training run.\n",
    "\n",
    "![*Figure 3: LangWatch Errors Evaluation.*](assets/copro/copro_lw_errors_eval.png)\n",
    "\n",
    "*Figure 3: LangWatch Experiment Detail of COPRO Optimization Final Evaluation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06300ac",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Thats it for this basic tutorial on instrumenting COPRO and LangWatch. Hope this has been helpful. You can check out the more advanced DSPy optimizers I've also written up on this site, they will be easier to understand if you have been able to follow along this one.\n",
    "\n",
    "If you have any questions or suggestions you can reach out to me on [Linkedin](https://www.linkedin.com/in/wsjames/) or on [X Platform](https://x.com/heylegacyguy)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
