# This script runs a post-evaluation of the 4.1-nano model with 
# the optimized DSPy module GrammaticalityClassifier against the 
# errors set generated by the pre-evaluation.

import os
import sys
import dspy
import langwatch
from dotenv import load_dotenv
from dspy.evaluate import Evaluate
from datasets import load_dataset
import pandas as pd
import csv
from optimizers.copro.dspy_grammatically import GrammaticalityClassifier, custom_metric

# Load CoLA dataset (Corpus of Linguistic Acceptability)
# Source https://huggingface.co/datasets/linxinyuan/cola/tree/main
dataset = load_dataset("glue", "cola")
train_df = dataset['train'].to_pandas()
val_df = dataset['validation'].to_pandas()

# Shuffle and truncate a subset of the training split for demonstration
train_df = train_df.sample(frac=1, random_state=23).head(10).reset_index(drop=True)
# Shuffle and truncate a subset of the test split for demonstration
val_df = val_df.sample(frac=1, random_state=23).head(10).reset_index(drop=True)

# Create trainset and devset
trainset = [dspy.Example(sentence=ex['sentence'], label=str(ex['label'])).with_inputs('sentence') for ex in train_df.to_dict(orient='records')]
devset = [dspy.Example(sentence=ex['sentence'], label=str(ex['label'])).with_inputs('sentence') for ex in val_df.to_dict(orient='records')]

# Keys are kept in .env file
load_dotenv()

# Configure DSPy - here using the cheapest model available 4.1-nano from OpenAI
lm = dspy.LM(model="openai/gpt-4.1-nano", api_key=os.environ.get("OPENAI_API_KEY"))
dspy.settings.configure(lm=lm)

# Load the optimized classifier state
loaded_classifier = GrammaticalityClassifier()  # Recreate the same program with the custom module.
loaded_classifier.load("./grammatically_optimized.json")

# Load the errors dataset
error_df = pd.read_csv("cola_grammar_errors.csv")

# Create a new devset from the mismatches
# (Assuming your columns are: sentence, true_label, predicted_label, is_correct)
errorset = [
    dspy.Example(sentence=row['sentence'], label=str(row['true_label'])).with_inputs('sentence')
    for _, row in error_df.iterrows()
]

# Evaluate the optimized classifier
optimized_evaluator = Evaluate(devset=errorset, num_threads=1, display_progress=True, display_table=10)

# Initialize Langwatch again
try:
    langwatch.setup(
        api_key=os.environ.get("LANGWATCH_API_KEY"),
        endpoint_url=os.environ.get("LANGWATCH_ENDPOINT")
    )
# If Langwatch setup fails exit so we do not run llm calls without observability
except Exception as e:
    print(f"LangWatch setup failed: {e}")
    sys.exit(1)  
langwatch.dspy.init(experiment="grammar-4.1-nano-copro-eval", optimizer=None, evaluator=optimized_evaluator) #<-- pass the evaluator for logging to LangWatch-Evaluations

# Run DSPy evaluation
optimized_evaluator(loaded_classifier, metric=custom_metric)